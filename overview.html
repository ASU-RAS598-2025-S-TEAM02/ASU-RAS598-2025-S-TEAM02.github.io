<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAS598 Team 02</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/style.css">
    <script type="module" src="modules/controlpanel/controlpanel.js"></script> <!-- Import control panel script -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> <!-- Include Mermaid library -->
</head>
<body class="d-flex flex-column min-vh-100">

<!-- Header -->
<div id="header-placeholder"></div>
<script src="modules/header/header.js"></script>

<!-- Main Content -->
<main class="flex-grow-1">
    <div class="container mt-5 pt-3">
        <style>
            h1, h2, h3 {
                font-weight: bold;
            }
        </style>
        <h1>Predator-Prey Dynamics Using ROS2 and TurtleBot 4 Lite</h1>
        <table class="table table-bordered">
            <tbody>
            <tr>
                <th scope="row">Team Members</th>
                <td>Michael Gross, Praneeth Boddeti, Abhishek Parsi</td>
            </tr>
            <tr>
                <th scope="row">Semester and Year</th>
                <td>Spring 2025</td>
            </tr>
            <tr>
                <th scope="row">University</th>
                <td>Arizona State University</td>
            </tr>
            <tr>
                <th scope="row">Class</th>
                <td>Experimentation and Deployment of Robotic Systems (RAS598)</td>
            </tr>
            <tr>
                <th scope="row">Professor</th>
                <td>Dr. Aukes</td>
            </tr>
            </tbody>
        </table>

        <h2>Project Plan</h2>
        <h3>Introduction</h3>
        <p> 
            This project aims to explore predator-prey dynamics using the TurtleBot 4 Lite in a controlled ROS Humble environment. The primary research question driving this work is:
            <br><br>
            <b>"How can the integration of ROS2-based control and sensor fusion improve the accuracy and stability of a predator-prey simulation in dynamic environments?"</b>
            <br><br>
            By leveraging ROS2's capabilities for real-time communication, sensor integration, and adaptive control, this project seeks to simulate biologically inspired behaviors in autonomous robots. The predator-prey model will demonstrate advanced navigation strategies, multi-agent coordination, and decision-making under dynamic conditions.
        </p>

        <div class="mermaid" style="text-align: center;">
            graph TD;
            subgraph Environment
            A((Predator)) -->|Chase| B((Prey))
            B -->|Evade| A
            end

            style Environment fill:#FFFFFF,stroke:#333333,stroke-width:2px,rx:10,ry:10;
            style A fill:#8C1D40,stroke:#333333,stroke-width:2px,color:#FFFFFF;
            style B fill:#FFC627,stroke:#333333,stroke-width:2px,color:#333333;
        </div>
        <figcaption style="text-align: center;">Figure 1: Predator-Prey Interaction</figcaption>
        <br>

        <h3>Sensor Integration</h3>
        <p>Sensor data will play a critical role in enabling autonomous behavior for both the predator and prey robots. The following sensors on the TurtleBot 4 Lite will be utilized:</p>
        <ul>
            <li><b>Depth Camera:</b> Employed to identify the prey's position in real-time, enabling the predator to dynamically adjust its trajectory for pursuit.</li>
            <li><b>IMU (Inertial Measurement Unit):</b> Ensures stable movement by compensating for environmental variations and assisting with precise turning angles.</li>
        </ul>
        <h4>Filtering and Sensor Data Conditioning</h3>
        <p>To ensure the accuracy and reliability of sensor data, the following filtering and conditioning techniques will be applied:</p>
        <ul>
            <li><b>LiDAR:</b> 
                <ul>
                    <li>Outlier removal using statistical filters to eliminate noise from spurious reflections.</li>
                    <li>Downsampling point clouds to reduce computational load while preserving essential spatial information.</li>
                </ul>
            </li>
            <li><b>IMU:</b> 
                <ul>
                    <li>Barlett and/or lowpass filtering to remove high-frequency noise from accelerometer and gyroscope data. (See <a href="assignment3.html">Filtering Analysis</a>)</li>
                    <li>Sensor fusion using complementary or Kalman filters to combine IMU data with other sources for improved orientation estimation.</li>
                    <li>Bias correction to account for drift in gyroscope readings over time. This will be implemented on the ESP32 IMU as a large proportion of the prey localization will be based on this data stream.</li>
                </ul>
            </li>
            <li><b>Computer Vision/Fiducial Tag Tracking:</b> 
                <ul>
                    <li>Applying moving average filters to smooth custom sensor data streams for improved accuracy.</li>
                    <li>Normalizing data to ensure compatibility with control algorithms and enhance decision-making processes.</li>
                    <li>Implementing object detection algorithms to improve tracking accuracy and responsiveness.</li>
                </ul>
            </li>
        </ul>
        <p>These techniques will ensure that the sensor data is robust, accurate, and suitable for real-time decision-making in dynamic environments.</p>

        <div class="mermaid" style="text-align: center; border-color: white;">
            graph TD;
            A[Initialize Sensors] -->|LiDAR: Obstacle detection & path planning| B[Sensor Data Collection]
            A -->|Depth Camera: Identify prey position| C[Sensor Data Collection]
            A -->|IMU: Movement stability & adjustments| D[Sensor Data Collection]
            B --> E[Data Logging & Analysis]
            C --> E
            D --> E
            E --> F[Algorithm Adjustment]
            F --> G[Testing & Refinement]
            G --> H[Real-Time Decision Making]

            subgraph "Real-Time Decision Making"
            H1[Predator: Dynamic tracking]
            H2[Prey: Sensor-based evasion]
            end

            H --> H1
            H --> H2

            style A fill:#8C1D40,stroke:#333333,stroke-width:2px,color:#FFFFFF,rx:10,ry:10;
            style B fill:#FFC627,stroke:#333333,stroke-width:2px,color:#333333,rx:10,ry:10;
            style C fill:#FFC627,stroke:#333333,stroke-width:2px,color:#333333,rx:10,ry:10;
            style D fill:#FFC627,stroke:#333333,stroke-width:2px,color:#333333,rx:10,ry:10;
            style E fill:#8C1D40,stroke:#333333,stroke-width:2px,color:#FFFFFF,rx:10,ry:10;
            style F fill:#8C1D40,stroke:#333333,stroke-width:2px,color:#FFFFFF,rx:10,ry:10;
            style G fill:#8C1D40,stroke:#333333,stroke-width:2px,color:#FFFFFF,rx:10,ry:10;
            style H fill:#8C1D40,stroke:#333333,stroke-width:2px,color:#FFFFFF,rx:10,ry:10;
            style H1 fill:#FFC627,stroke:#333333,stroke-width:2px,color:#333333,rx:10,ry:10;
            style H2 fill:#FFC627,stroke:#333333,stroke-width:2px,color:#333333,rx:10,ry:10;
        </div>
        <figcaption style="text-align: center;">Figure 2: Sensor Integration Flowchart</figcaption>
        <br>

        <h4>Data Logging and Analysis (Experimentation Phase)</h4>
        <ul>
            <li>Sensor data will be logged using ROSBag for replaying scenarios and debugging.</li>
            <li>Movement algorithms will be refined based on logged data to optimize speed, turn rates, and responsiveness to obstacles.</li>
            <li>Pursuit and evasion strategies will be iteratively improved by analyzing performance metrics such as reaction time, accuracy of detection, and path efficiency.</li>
        </ul>

        <h3>Interaction & Interface Design</h3>
        <p>To facilitate interaction with the system and monitor performance, the following interface components will be developed:</p>

        <h4>Visualization</h4>
        <p>An RViz2 dashboard will display real-time sensor data, robot paths, and tracking information. This visualization will provide insights into how each robot perceives its environment and executes its strategies.</p>

        <h4>Control Panel</h4>
        <p>The control panel will allow users to manage IMU and camera feeds, as well as interact with the IMU for data visualization and analysis.</p>

        <control-panel></control-panel>
        <figcaption style="text-align: center;">Figure 3: Sample Control Panel Interface</figcaption>
        <br>

        <h4>Web-Based Dashboard or GUI</h4>
        <p>A web-based interface or ROS2-based GUI will allow users to interact with the robots remotely in real time.</p>

        <h4>Data Logging</h4>
        <p>Performance metrics such as chase success rate, evasion duration, and path efficiency will be logged in CSV or ROSBag files for post-simulation analysis.</p>
    

        <div class="mermaid" style="text-align: center;">
            graph TD

            %% IMU Logger
            subgraph IMU_Logger
                A[IMUDataLogger<br>ROS 2 Node]
            end

            B[<b>GraphView Widget</b><br>Plots IMU Data<br><b>Buttons:</b><br>- Subscribe Camera<br>- Subscribe Map<br>- Query Data at Time]
            
            %% Graph View + Telemetry Subscriptions
            subgraph Telemetry_UI
                C[RPI IMU Callback<br>Updates RPI IMU Buffer]
                D[ESP IMU Callback<br>Updates ESP IMU Buffer]
            end
            
            %% Camera and Map Windows
            subgraph Visualization_Windows
                E[CameraWindow<br>Displays Live Feed]
                F[MapWindow<br>Shows Occupancy Grid<br>Draws Pose via QPainter]
            end
            
            %% Connections
            A --> B
            B -->|/rpi_05/imu| C
            B -->|/esp/imu_data| D
            B -->|/rpi_05/oakd/image/preview| E
            B --> |/rpi_05/pose| F
            B --> |/rpi_05/map| F
            B --> |/rpi_05/scan| F
            
            %% Styles
            style A fill:#8C1D40,stroke:#333,stroke-width:2px,color:#FFFFFF,rx:10,ry:10
            style B fill:#FFC627,stroke:#333,stroke-width:2px,color:#000,rx:10,ry:10
            style C fill:#D6EAF8,stroke:#333,stroke-width:2px,color:#000,rx:10,ry:10
            style D fill:#D6EAF8,stroke:#333,stroke-width:2px,color:#000,rx:10,ry:10
            style E fill:#D6EAF8,stroke:#333,stroke-width:2px,color:#000,rx:10,ry:10
            style F fill:#D6EAF8,stroke:#333,stroke-width:2px,color:#000,rx:10,ry:10
        </div>
        <figcaption style="text-align: center;">Figure 4: IMU Data Flow Diagram for GUI Interface</figcaption>
        <br>

        <h3>Control and Autonomy</h3>
        <p>The control system integrates sensor feedback with decision-making algorithms to enable real-time autonomous behavior for both systems:</p>

        <h4>Predator Logic</h4>
        <ul>
            <li>The predator robot will use a pursuit algorithm (ie. proportional navigation) that dynamically updates its trajectory based on the prey's last known position.</li>
            <li>If the prey is lost from view, a search pattern algorithm will be activated until the target is reacquired.</li>
            <li>Obstacle avoidance logic ensures smooth navigation while pursuing the prey.</li>
        </ul>

        <h4>Prey Logic</h4>
        <ul>
            <li>The prey robot will evade the predator via teleoperated motion.</li>
            <li>Randomization in movement patterns will prevent predictable behavior, making it more challenging for the predator to capture it.</li>
            <li>Sensor feedback will guide path adjustments to navigate around obstacles effectively.</li>
        </ul>

        <h4>ROS2 RQT Graph</h4>
        <p>The graph provides a visual representation of the nodes and topics in the system, showcasing the communication flow between various components.</p>
        <div style="display: flex; justify-content: center;">
            <img src="assets/ros/rqt_graph.png" alt="ROS2 RQT Graph" style="max-width: 100%; border: 1px solid #ccc; border-radius: 5px;">
        </div>
        <figcaption style="text-align: center;">Figure 6: ROS2 RQT Graph</figcaption>
        <br>


        <h3>Combining Data Sources for Low-Level and High-Level Decisions</h3>
        <p>The integration of data from multiple sensors is essential for enabling both low-level feedback control and high-level autonomy in the predator-prey simulation. This approach ensures that the system can respond to real-time environmental changes while executing strategic behaviors:</p>

        <h4>Low-Level Decisions (Feedback Control)</h4>
        <ul>
            <li><b>Sensor Data Acquisition:</b> Raw data from LiDAR, depth cameras, and IMUs is collected and processed in real time.</li>
            <li><b>Filtering and Noise Reduction:</b> Techniques such as statistical filters, median filters, and low-pass filters are applied to ensure clean and reliable data.</li>
            <li><b>Immediate Actions:</b> Low-level control handles tasks like obstacle avoidance, trajectory adjustments, and maintaining stability.</li>
        </ul>

        <h4>High-Level Decisions (Autonomy)</h4>
        <ul>
            <li><b>Path Planning:</b> Processed sensor data is used to compute optimal paths for pursuit or evasion.</li>
            <li><b>Behavioral Decision-Making:</b> Strategies such as search patterns, pursuit algorithms, and evasion tactics are implemented at this level.</li>
            <li><b>Multi-Agent Coordination:</b> Communication between the predator and prey systems ensures synchronized interactions.</li>
        </ul>

        <h4>Integration Strategy</h4>
        <p>The integration of these layers is achieved through a modular ROS2 architecture:</p>
        <ul>
            <li><b>ROS2 Nodes:</b> Dedicated nodes handle specific tasks, such as sensor data processing, control commands, and decision-making.</li>
            <li><b>Data Flow:</b> Low-level nodes publish filtered sensor data to topics, which are then subscribed to by high-level nodes for further processing.</li>
            <li><b>Feedback Loop:</b> High-level decisions are translated into low-level commands, creating a continuous feedback loop for adaptive behavior.</li>
        </ul>

        <div class="mermaid" style="text-align: center;">
            graph TD

            %% Sensor Inputs
            A1[IMU - RPI]
            A2[IMU - ESP]
            A3[Camera Feed]
            A5[Pose Data]
            
            %% Data Processing
            B1[IMU Handler<br>Orientation Estimation<br>+ Filtering]
            B2[Camera Processor<br>Object Detection]
            B4[Pose Handler<br>Estimate Pose<br>+ Smoothing]
            
            %% Control Layers
            C1[Low-Level Control<br>Velocity & Balance<br>Closed-loop Logic]
            C2[High-Level Autonomy<br>Navigation & Planning]
            
            %% Data Logging
            D1[GraphView<br>Visualize & Log Data]
            
            %% Connections
            A1 --> B1
            A2 --> B1
            A3 --> B2
            A5 --> B4
            
            B1 --> C1
            B2 --> C2
            B4 --> C2
            
            B1 --> D1
            B2 --> D1
            B4 --> D1
            
            %% Styles
            style A1 fill:#FFC627,stroke:#333,stroke-width:2px,color:#000
            style A2 fill:#FFC627,stroke:#333,stroke-width:2px,color:#000
            style A3 fill:#FFC627,stroke:#333,stroke-width:2px,color:#000
            style A5 fill:#FFC627,stroke:#333,stroke-width:2px,color:#000
            
            style B1 fill:#8C1D40,stroke:#333,stroke-width:2px,color:#fff
            style B2 fill:#8C1D40,stroke:#333,stroke-width:2px,color:#fff
            style B4 fill:#8C1D40,stroke:#333,stroke-width:2px,color:#fff
            
            style C1 fill:#000000,stroke:#333,stroke-width:2px,color:#fff
            style C2 fill:#000000,stroke:#333,stroke-width:2px,color:#fff
            
            style D1 fill:#D6EAF8,stroke:#333,stroke-width:2px,color:#000
        </div>
        <figcaption style="text-align: center;">Figure 7: Simulation Workflow</figcaption>
        <br>

        <p>This layered approach ensures that the system can respond quickly to environmental changes while maintaining strategic objectives, enabling a seamless blend of reactive and deliberative behaviors.</p>

        <p>Both systems will operate using ROS2 control nodes that process sensor data and execute movement commands in real time.</p>


        <h3>Preparation Needs</h3>
        <ul>
            <li><b>ROS2 Navigation Stack:</b> For efficient path planning and obstacle avoidance.</li>
            <li><b>Real-Time Object Tracking:</b> To enhance detection accuracy using depth cameras.</li>
            <li><b>PID Control:</b> For precise movement control during pursuit or evasion.</li>
            <li><b>Sensor Fusion Techniques:</b> To combine data from multiple sensors (LiDAR, depth camera, IMU) for robust decision-making.</li>
            <li><b>Multi-Agent Coordination Strategies:</b> To manage communication between robots effectively.</li>
        </ul>
        <p>Class coverage of advanced ROS2 control strategies, behavior-based robotics principles, effective sensor integration methods, and robust path-planning algorithms would significantly support this project’s goals.</p>


        <h3>Final Demonstration</h3>
        <ul>
            <li>The predator robot will use real-time sensor data to track and pursue the prey based on continuously updated positions.</li>
            <li>The prey robot will leverage sensor feedback to evade capture for three minutes by navigating dynamically around obstacles.</li>
            <li>One TurtleBot 4 Lite robot (predator) and an IMU-equipped ESP32 (prey) will operate within a 3m x 3m area containing obstacles (e.g., classroom furniture).</li>
            <li>A projector will display RViz2 visualizations of sensor data and robot paths for audience observation.</li>
        </ul>
        <p>This setup will showcase how adaptive control systems driven by sensory inputs can influence robot behavior in real-world scenarios. It highlights how biologically inspired behaviors can be modeled using robotics frameworks like ROS2 while demonstrating advanced multi-agent coordination.</p>

        <h3>Impact</h3>
        <p>This project has significant potential to enhance both personal learning outcomes and broader educational value:</p>
        <ol>
            <li>It offers hands-on experience with advanced robotics concepts such as multi-agent communication, trajectory planning, adaptive algorithms, SLAM integration, and biologically inspired behavior modeling.</li>
            <li>It bridges theoretical knowledge with practical applications by simulating real-world dynamics through robotics systems.</li>
            <li>It fosters interdisciplinary learning across robotics, artificial intelligence, biology-inspired systems, and control theory.</li>
            <li>The project could serve as a foundation for course development by providing an engaging example of complex theoretical concepts applied in practice—promoting critical thinking, problem-solving skills, and technical expertise essential for advancing robotics education.</li>
        </ol>

        <h3>Future Work</h3>
        <p>During the development phase, we successfully implemented SLAM in isolation. Using the TurtleBot 4 Lite's LiDAR and depth camera, we were able to generate accurate occupancy grids and real-time maps of the environment. The system performed well in controlled scenarios, with minimal drift and consistent map updates.</p>

        <p>However, when attempting to run SLAM alongside the localization and graphical display of the predator and prey, we encountered several challenges:</p>
        <ul>
            <li><b>Service Launch Issues:</b> The TurtleBot 4 services were not always launching successfully, and at times, we were unable to echo the topics to verify their functionality.</li>
            <li><b>SLAM Launch Failures:</b> The SLAM process occasionally failed to fully launch due to communication issues between the Create 3 base and the Raspberry Pi.</li>
        </ul>

        <p>To address these issues, we explored the following solutions:</p>
        <ul>
            <li>Investigating and resolving the root causes of service launch failures by reviewing logs and ensuring proper initialization sequences.</li>
            <li>Improving the communication reliability between the Create 3 base and the Raspberry Pi by optimizing network configurations and verifying hardware connections.</li>
        </ul>

        <p>However, we could not get it fully integrated in time for the submission of this project.</p>

        <div style="display: flex; justify-content: center;">
            <video controls width="75%" style="border: 1px solid #ccc; border-radius: 5px;">
            <source src="assets/slam_data/slam_demonstration.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video>
        </div>
        <figcaption style="text-align: center;">Figure 5: SLAM Demonstration Video</figcaption>
        <br>


        <h3>Advising</h3>
        <p>Dr. Aukes has been identified as the faculty advisor for this project. While initial discussions about this idea have taken place during office hours, we plan to formally request his mentorship via email. His guidance is expected to provide access to specialized hardware resources (e.g., TurtleBot 4 Lite, ESP32, IMU), expertise in robotics systems design using ROS2 frameworks, and valuable feedback on implementing advanced control strategies.</p>


        <h3>Weekly Milestones</h3>
        <div class="mermaid">
            gantt
            dateFormat  YYYY-MM-DD
            section Planning
            Ideation                  :done,    des1, 2025-02-23, 2025-03-01
            section Development
            Hardware Integration      :         des2, 2025-03-02, 2025-03-05
            Interface Development     :         des3, 2025-03-05, 2025-03-10
            Sensors                   :         des4, 2025-03-10, 2025-03-15
            Controls & Autonomy       :         des5, 2025-03-15, 2025-04-07
            section Testing
            Real-World Testing        :         des7, 2025-04-07, 2025-04-20
            section Finalization
            Data Analysis             :         des8, 2025-04-20, 2025-04-20
            Final Report              :         des9, 2025-04-20, 2025-05-05
            Presentation Preparation  :         des10, 2025-04-23, 2025-05-01
        </div>
        <figcaption style="text-align: center;">Figure 8: Gantt Chart of Weekly Milestones</figcaption>
        <br>

    </div>

</main>

<!-- Footer -->
<div id="footer-placeholder"></div>
<script src="modules/footer/footer.js"></script>


<!-- Scripts -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>

</body>
</html>